{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we've struggled with for a long time is having a persistent online reference point for the different EPA Ecoregion datasets that we use in biogeography. Data files are online and released via [EPA's web site](https://www.epa.gov/eco-research/ecoregions), but that's been problematic at times when EPA has either taken things down for a while or moved them. Ideally, we would have some kind of persistent ID that we could always reference, that ID would be resolvable with code so we could reference it and get additional information for software systems, and we would have a place to rally value-added properties and linkages over time. This seemed like another reasonable use case for WikiData in some ongoing research I'm doing on uses of that system for other purposes.\n",
    "\n",
    "In looking through WikiData any hint of these records already being online, I found items for [Level III Ecoregions](https://www.wikidata.org/wiki/Q52111338) and [Level IV Ecoregions](https://www.wikidata.org/wiki/Q52111409). However, there are only 20 of 182 level 3 ecoregions actually created as instances of that item and 44 level 4 ecoregions. They were created by the same user some time ago. I reached out for clarification on intent to [User:TimK MSI](https://www.wikidata.org/wiki/User:TimK_MSI) who came up with a fairly reasonable start to a data model with items like [Rocky Mountains](https://www.wikidata.org/wiki/Q67197232). This includes some variation but mostly has the following elements:\n",
    "\n",
    "* Link to the country (all US as the focus has been on the CONUS mapping)\n",
    "* Links to US States the ecoregion intersects\n",
    "* Representative point location as coordinates\n",
    "* Some have \"has part\" links to Level 4 ecoregion items\n",
    "* Some have relationships to similarly named items showing a connection to things like physiographic features\n",
    "\n",
    "One issue I see is that the current items use a series ordinal qualifier on the Level III Ecoregion valued class property as a way of capturing an ID that corresponds to the numbering system specific to the CONUS derivation of the North American Ecoregions dataset. A better approach would be to formalize the actual identifier properties and assign the IDs there. I will work through that process as it will be fun to see how that plays out in the WikiData community, but that will be a bit more involved. Having the identifiers on board the WikiData items themselves will allow existing information systems using things like \"US_L3CODE\" as a key to readily query WikiData with known information.\n",
    "\n",
    "Data prep in this case is pretty straightforward but will involve a number of steps to translate all of the relevant data into appropriate WikiData items and statements. To be fully complete, we should incorporate all ecoregion concepts as defined by Omernik and others in the original work from Level I through Level IV and harmonizing the CONUS derivative of L3 ecoregions with the North American ecoregions by linking their IDs. For this purpose, we are going to need to get all of the individual source data for level 1-4 ecoregions and stitch everything together.\n",
    "\n",
    "I built out a separate process to develop a political boundaries dataset for all three countries covering the North America ecoregions. This is loaded to a PostGIS instance for spatial processing to obtain the intersections between ecoregion boundaries and political boundaries of interest.\n",
    "\n",
    "I looked into two other things I would have liked to do with these data: getting the full multipolygon geometry online somewhere for each ecoregion boundary and loading the ecoregion description information online that is currently locked up in a downloadable PDF. Unfortunately, both of those objectives are a little problematic.\n",
    "\n",
    "Sticking within the Wikiverse and according to documentation, we should be able to load the geometry to Wikimedia Commons items and reference them with the geoshape property. However, after looking into where and how this has been done in other items, I really didn't see any added value to that approach. Any programming against the system would have to go through some pretty circuitous routes to put everything together for use in analysis or visualization. What we would want is some kind of immediate access to a geospatial service from the WikiData item that would return the geospatial part of the data in a variety of formats. Unfortunately, I wasn't able to track down anything close to an authoritative, stable source for that, even from our own geospatial servers in USGS. An interesting phenomenon seen online are numerous cases where people have clipped out an ecoregion subset of one kind or another at State boundaries, probably for ecosystem context and analysis within the State. I've yet to find one of those that shows a careful provenance trace to where the information comes from or how it was processed.\n",
    "\n",
    "I should be able to do something interesting with the ecoregion descriptions, but I need to think about it a while longer. The original source is in a PDF provided from the same source as the data and released in 2011 by the are a little bit in a government report type of release from the Commission for Environmental Cooperation. That document has a Creative Commons-No Derivatives license, which essentially prohibits taking pieces of it out and putting them somewhere else. However, EPA also released a Word version of the basic descriptive content that should be completely public domain, and I could conceivably build some code to scrape the individual descriptions and create Wikimedia Commons articles for them as a reference point. That would give us a persistent identity and the content to work with. But there are a number of Wikipedia articles that mostly describe some of the same basic ecosystems, albeit from a different perspective with Omernik's work just one source. I haven't figured out the most elegant way to link these various artifacts together just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import get_source_df, lookup_wd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from geoalchemy2 import Geometry, WKTElement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built some helper functions to deal with common chores in this workflow, including functions to retrieve source data from FTP or HTTP URLs, run some common transformation steps, and return geodataframes for processing. The following codeblock retrieves all 5 ecoregion source datasets as shapefiles and runs basic transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 s, sys: 509 ms, total: 4.47 s\n",
      "Wall time: 4.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "na_er1 = get_source_df(\n",
    "    source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/na_cec_eco_l1.zip\",\n",
    "    add_columns=[('source', 'NA_CEC_Eco_Level1')],\n",
    "    target_file_name=\"NA_CEC_Eco_Level1.shp\"\n",
    ")\n",
    "\n",
    "na_er2 = get_source_df(\n",
    "    source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/na_cec_eco_l2.zip\",\n",
    "    add_columns=[('source', 'NA_CEC_Eco_Level2')],\n",
    "    target_file_name=\"NA_CEC_Eco_Level2.shp\"\n",
    ")\n",
    "\n",
    "na_er3 = get_source_df(\n",
    "    source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/NA_CEC_Eco_Level3.zip\",\n",
    "    add_columns=[('source', 'NA_CEC_Eco_Level3')]\n",
    ")\n",
    "\n",
    "conus_er3 = get_source_df(\n",
    "    source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\",\n",
    "    add_columns=[('source', 'US_Eco_Level3')]\n",
    ")\n",
    "\n",
    "conus_er4 = get_source_df(\n",
    "    source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip\",\n",
    "    add_columns=[('source', 'US_Eco_Level4')],\n",
    "    target_file_name=\"us_eco_l4_no_st.shp\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reproject every dataset to a common CRS for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 s, sys: 752 ms, total: 20.2 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "na_er1 = na_er1.to_crs({'init': 'epsg:4326'})\n",
    "na_er2 = na_er2.to_crs({'init': 'epsg:4326'})\n",
    "na_er3 = na_er3.to_crs({'init': 'epsg:4326'})\n",
    "conus_er3 = conus_er3.to_crs({'init': 'epsg:4326'})\n",
    "conus_er4 = conus_er4.to_crs({'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this codeblock, I simplify and standardize the data schemas of each ecoregion source dataset so to make things simpler for further processing into WikiData items. One of the main things here is setting up each level of the inherently hierarchical system with the ecoregion identifiers that will eventually become links to the related items in WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_er1[\"contextual_identifier\"] = na_er1.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE}\", axis=1)\n",
    "na_er1[\"common_name\"] = na_er1.apply(lambda row: row.NA_L1NAME, axis=1)\n",
    "na_er1.drop(\n",
    "    [c for c in list(na_er1.columns) if c not in [\"source\",\"contextual_identifier\",\"common_name\",\"geometry\"]], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "na_er2[\"contextual_identifier\"] = na_er2.apply(lambda row: f\"NA_L2CODE:{row.NA_L2CODE}\", axis=1)\n",
    "na_er2[\"common_name\"] = na_er2.apply(lambda row: row.NA_L2NAME, axis=1)\n",
    "na_er2[\"part_of_identifiers\"] = na_er2.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE}\", axis=1)\n",
    "na_er2.drop(\n",
    "    [c for c in list(na_er2.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "na_er3[\"contextual_identifier\"] = na_er3.apply(lambda row: f\"NA_L1CODE:{row.NA_L3CODE}\", axis=1)\n",
    "na_er3[\"common_name\"] = na_er3.apply(lambda row: row.NA_L3NAME, axis=1)\n",
    "na_er3[\"part_of_identifiers\"] = na_er3.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE};NA_L2CODE:{row.NA_L2CODE}\", axis=1)\n",
    "na_er3.drop(\n",
    "    [c for c in list(na_er3.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "conus_er3[\"contextual_identifier\"] = conus_er3.apply(lambda row: f\"US_L3CODE:{row.US_L3CODE}\", axis=1)\n",
    "conus_er3[\"common_name\"] = conus_er3.apply(lambda row: row.US_L3NAME, axis=1)\n",
    "conus_er3[\"part_of_identifiers\"] = conus_er3.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE};NA_L2CODE:{row.NA_L2CODE};NA_L3CODE:{row.NA_L3CODE}\", axis=1)\n",
    "conus_er3.drop(\n",
    "    [c for c in list(conus_er3.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "conus_er4[\"contextual_identifier\"] = conus_er4.apply(lambda row: f\"US_L4CODE:{row.US_L4CODE}\", axis=1)\n",
    "conus_er4[\"common_name\"] = conus_er4.apply(lambda row: row.US_L4NAME, axis=1)\n",
    "conus_er4[\"part_of_identifiers\"] = conus_er4.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE};NA_L2CODE:{row.NA_L2CODE};NA_L3CODE:{row.NA_L3CODE};US_L3CODE:{row.US_L3CODE}\", axis=1)\n",
    "conus_er4.drop(\n",
    "    [c for c in list(conus_er4.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this codeblock, I concatenate all ecoregion source data together into one dataframe now that everything has the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.6 ms, sys: 335 µs, total: 6.94 ms\n",
      "Wall time: 8.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoregions = gpd.GeoDataFrame(\n",
    "    pd.concat(\n",
    "        [\n",
    "            na_er1, \n",
    "            na_er2, \n",
    "            na_er3, \n",
    "            conus_er3,\n",
    "            conus_er4\n",
    "        ], \n",
    "        ignore_index=True, \n",
    "        sort=False\n",
    "    )\n",
    ")\n",
    "ecoregions.crs = na_er1.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subsequent steps, I need to get a representational point for each ecoregion unit and run intersections with administrative boundaries. To make this a bit simpler in providing values grouped/centered on each distinct ecoregion unit, I run a dissolve and group by the contextual_identifier created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 6.24 s, total: 2min 46s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simplified_ecoregions = ecoregions.dissolve(by='contextual_identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I introduce x and y coordinates for a representational point to add to the WikiData items, proper case the ecoregion name for the WikiData label, and run a lookup on a previously built WikiData reference set where I pulled in all current instances of Level 3 and 4 CONUS ecoregions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.08 s, sys: 182 ms, total: 6.27 s\n",
      "Wall time: 6.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simplified_ecoregions[\"x\"] = simplified_ecoregions.centroid.x\n",
    "simplified_ecoregions[\"y\"] = simplified_ecoregions.centroid.y\n",
    "\n",
    "simplified_ecoregions['common_name'] = simplified_ecoregions['common_name'].str.title()\n",
    "simplified_ecoregions[\"wikidata_id\"] = simplified_ecoregions.apply(lambda row: lookup_wd(\"Ecoregions\", row[\"common_name\"], None), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For processing in PostGIS, I need to split up to like geometry types - POLYGON and MULTIPOLYGON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 609 ms, total: 24.7 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoregions_multi = simplified_ecoregions.apply(copy.deepcopy)\n",
    "ecoregions_multi.drop(\n",
    "    ecoregions_multi[ecoregions_multi.geometry.geom_type == \"Polygon\"].index, \n",
    "    inplace=True\n",
    ")\n",
    "simplified_ecoregions.drop(\n",
    "    simplified_ecoregions[simplified_ecoregions.geometry.geom_type == \"MultiPolygon\"].index, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "ecoregions_multi['geom'] = ecoregions_multi['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "simplified_ecoregions['geom'] = simplified_ecoregions['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "\n",
    "ecoregions_multi.drop('geometry', 1, inplace=True)\n",
    "simplified_ecoregions.drop('geometry', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I load the two datasets of like geometry to PostGIS tables for spatial processing to find intersections with administrative boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 1.02 s, total: 3.17 s\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db_connection_url = f\"postgres://{os.environ.get('PG_USER')}:{os.environ.get('PG_PASS')}@{os.environ.get('PG_HOST')}:{os.environ.get('PG_PORT')}/ecoregions\";\n",
    "db_engine = create_engine(db_connection_url)\n",
    "\n",
    "simplified_ecoregions.to_sql(\n",
    "    \"ecoregions_p\", \n",
    "    db_engine, \n",
    "    if_exists='replace', \n",
    "    index=True,\n",
    "    dtype={'geom': Geometry('POLYGON', srid=4326)}\n",
    ")\n",
    "\n",
    "ecoregions_multi.to_sql(\n",
    "    \"ecoregions_m\", \n",
    "    db_engine, \n",
    "    if_exists='replace', \n",
    "    index=True,\n",
    "    dtype={'geom': Geometry('MULTIPOLYGON', srid=4326)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this part of things out for further processing into WikiData items, I used processes directly in PostGIS to have it handle the more intensive spatial processing. This involved creating a materialized view containing the names and identifiers of intersected boundaries that can be used to generate WikiData links (statements/claims) to relevant items. This was handled with the following SQL.\n",
    "\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_m AS b, ecoregions_m AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "    UNION\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_m AS b, ecoregions_p AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "    UNION\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_p AS b, ecoregions_m AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "    UNION\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_p AS b, ecoregions_p AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "\n",
    "Note: I do need to eventually come up with a more elegant and performant way of handling this type of linking functionality using cloud resources somewhere as it is a fairly common task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With intersections discovered, we can bring everything together into one combined dataset that contains the essential information we will use to create the WikiData items. The following codeblock retrieves the intersection data from PostGIS and sets up a simple function to retrieve the WikiData ID values for those admin entities that I previously assembled whose geometry intersects with ecoregion boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intersections = pd.read_sql(\"SELECT * FROM intersections\", db_engine)\n",
    "\n",
    "def boundary_intersects_list(contextual_identifier):\n",
    "    intersects = df_intersections.loc[df_intersections.contextual_identifier == contextual_identifier][\"wikidata_id\"].to_list()\n",
    "    \n",
    "    return intersects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I bring the properties (minus geometry) together again for all ecoregion units across all 5 datasets and apply a couple of processes to prep the data for working up into WikiData items. These include splitting the list of other ecoregion identifiers that will become \"part of\" relationships in WikiData into a list object and applying our function to provide a list of intersecting administrative boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ecoregions = '''SELECT \n",
    "    contextual_identifier,\n",
    "    wikidata_id,\n",
    "    source, \n",
    "    common_name, \n",
    "    part_of_identifiers, \n",
    "    x, \n",
    "    y \n",
    "    FROM ecoregions_m\n",
    "    UNION\n",
    "    SELECT \n",
    "    contextual_identifier, \n",
    "    wikidata_id,\n",
    "    source, \n",
    "    common_name, \n",
    "    part_of_identifiers, \n",
    "    x, \n",
    "    y \n",
    "    FROM ecoregions_p\n",
    "    '''\n",
    "\n",
    "df_ecoregions = pd.read_sql(q_ecoregions, db_engine)\n",
    "df_ecoregions['part_of_identifiers'] = df_ecoregions['part_of_identifiers'].apply(lambda x: x.split(';') if x is not None else x)\n",
    "df_ecoregions['boundary_intersects'] = df_ecoregions['contextual_identifier'].apply(lambda x: boundary_intersects_list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have everything we need to stub out functional WikiData items for each ecoregion unit in all 3 North American classification systems and the 2 for CONUS. This includes linkages we will be able to create to state/province administrative units across all three countries (and each of the countries) along with US counties. We have a representational point location for basic map orientation. And we will be able to build out \"part of/has part\" relationships between relevant ecoregion units once we have the items created and IDs generated in WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextual_identifier</th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>source</th>\n",
       "      <th>common_name</th>\n",
       "      <th>part_of_identifiers</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>boundary_intersects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US_L4CODE:50aa</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Menominee-Drummond Lakeshore</td>\n",
       "      <td>[NA_L1CODE:5, NA_L2CODE:5.2, NA_L3CODE:5.2.1, ...</td>\n",
       "      <td>-85.871227</td>\n",
       "      <td>45.918197</td>\n",
       "      <td>[Q1166, Q1537, Q491882, Q491121, Q491857, Q113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US_L4CODE:51g</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Door Peninsula</td>\n",
       "      <td>[NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.4, ...</td>\n",
       "      <td>-87.327961</td>\n",
       "      <td>44.916542</td>\n",
       "      <td>[Q1537, Q490550, Q932951]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US_L4CODE:63b</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Chesapeake-Pamlico Lowlands And Tidal Marshes</td>\n",
       "      <td>[NA_L1CODE:8, NA_L2CODE:8.5, NA_L3CODE:8.5.1, ...</td>\n",
       "      <td>-76.286714</td>\n",
       "      <td>36.985518</td>\n",
       "      <td>[Q1454, Q1370, Q1391, Q497817, Q501263, Q50128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US_L4CODE:59c</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Southern New England Coastal Plains And Hills</td>\n",
       "      <td>[NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.7, ...</td>\n",
       "      <td>-72.398028</td>\n",
       "      <td>41.656111</td>\n",
       "      <td>[Q771, Q1384, Q1387, Q779, Q54234, Q54066, Q54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NA_L1CODE:9.4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NA_CEC_Eco_Level3</td>\n",
       "      <td>Cross Timbers</td>\n",
       "      <td>[NA_L1CODE:9, NA_L2CODE:9.4]</td>\n",
       "      <td>-97.506560</td>\n",
       "      <td>33.588485</td>\n",
       "      <td>[Q1439, Q1558, Q1649, Q108424, Q109825, Q48460...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  contextual_identifier wikidata_id             source  \\\n",
       "0        US_L4CODE:50aa        None      US_Eco_Level4   \n",
       "1         US_L4CODE:51g        None      US_Eco_Level4   \n",
       "2         US_L4CODE:63b        None      US_Eco_Level4   \n",
       "3         US_L4CODE:59c        None      US_Eco_Level4   \n",
       "4       NA_L1CODE:9.4.5        None  NA_CEC_Eco_Level3   \n",
       "\n",
       "                                     common_name  \\\n",
       "0                   Menominee-Drummond Lakeshore   \n",
       "1                                 Door Peninsula   \n",
       "2  Chesapeake-Pamlico Lowlands And Tidal Marshes   \n",
       "3  Southern New England Coastal Plains And Hills   \n",
       "4                                  Cross Timbers   \n",
       "\n",
       "                                 part_of_identifiers          x          y  \\\n",
       "0  [NA_L1CODE:5, NA_L2CODE:5.2, NA_L3CODE:5.2.1, ... -85.871227  45.918197   \n",
       "1  [NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.4, ... -87.327961  44.916542   \n",
       "2  [NA_L1CODE:8, NA_L2CODE:8.5, NA_L3CODE:8.5.1, ... -76.286714  36.985518   \n",
       "3  [NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.7, ... -72.398028  41.656111   \n",
       "4                       [NA_L1CODE:9, NA_L2CODE:9.4] -97.506560  33.588485   \n",
       "\n",
       "                                 boundary_intersects  \n",
       "0  [Q1166, Q1537, Q491882, Q491121, Q491857, Q113...  \n",
       "1                          [Q1537, Q490550, Q932951]  \n",
       "2  [Q1454, Q1370, Q1391, Q497817, Q501263, Q50128...  \n",
       "3  [Q771, Q1384, Q1387, Q779, Q54234, Q54066, Q54...  \n",
       "4  [Q1439, Q1558, Q1649, Q108424, Q109825, Q48460...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ecoregions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save out the final processed dataset as a Python data structure for processing to WikiData in another step. At this point, we no longer need our PostGIS intermediary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "outfile = open(\"data_cache/ecoregions.pkl\", \"wb\")\n",
    "pickle.dump(df_ecoregions.to_dict(orient=\"records\"), outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
