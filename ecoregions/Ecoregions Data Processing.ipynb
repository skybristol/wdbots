{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we've struggled with for a long time is having a persistent online reference point for the different EPA Ecoregion datasets that we use in biogeography. Data files are online and released via [EPA's web site](https://www.epa.gov/eco-research/ecoregions), but that's been problematic at times when EPA has either taken things down for a while or moved them. Ideally, we would have some kind of persistent ID that we could always reference, that ID would be resolvable with code so we could reference it and get additional information for software systems, and we would have a place to rally value-added properties and linkages over time. This seemed like another reasonable use case for WikiData in some ongoing research I'm doing on uses of that system for other purposes.\n",
    "\n",
    "In looking through WikiData any hint of these records already being online, I found items for [Level III Ecoregions](https://www.wikidata.org/wiki/Q52111338) and [Level IV Ecoregions](https://www.wikidata.org/wiki/Q52111409). However, there are only 20 of 182 level 3 ecoregions actually created as instances of that item and 44 level 4 ecoregions. They were created by the same user some time ago. I reached out for clarification on intent to [User:TimK MSI](https://www.wikidata.org/wiki/User:TimK_MSI) who came up with a fairly reasonable start to a data model with items like [Rocky Mountains](https://www.wikidata.org/wiki/Q67197232). This includes some variation but mostly has the following elements:\n",
    "\n",
    "* Link to the country (all US as the focus has been on the CONUS mapping)\n",
    "* Links to US States the ecoregion intersects\n",
    "* Representative point location as coordinates\n",
    "* Some have \"has part\" links to Level 4 ecoregion items\n",
    "* Some have relationships to similarly named items showing a connection to things like physiographic features\n",
    "\n",
    "One issue I see is that the current items use a series ordinal qualifier on the Level III Ecoregion valued class property as a way of capturing an ID that corresponds to the numbering system specific to the CONUS derivation of the North American Ecoregions dataset. A better approach would be to formalize the actual identifier properties and assign the IDs there. I will work through that process as it will be fun to see how that plays out in the WikiData community, but that will be a bit more involved. Having the identifiers on board the WikiData items themselves will allow existing information systems using things like \"US_L3CODE\" as a key to readily query WikiData with known information.\n",
    "\n",
    "Data prep in this case is pretty straightforward but will involve a number of steps to translate all of the relevant data into appropriate WikiData items and statements. To be fully complete, we should incorporate all ecoregion concepts as defined by Omernik and others in the original work from Level I through Level IV and harmonizing the CONUS derivative of L3 ecoregions with the North American ecoregions by linking their IDs. For this purpose, we are going to need to get all of the individual source data for level 1-4 ecoregions and stitch everything together.\n",
    "\n",
    "I built out a separate process to develop a political boundaries dataset for all three countries covering the North America ecoregions. This is loaded to a PostGIS instance for spatial processing to obtain the intersections between ecoregion boundaries and political boundaries of interest.\n",
    "\n",
    "I looked into two other things I would have liked to do with these data: getting the full multipolygon geometry online somewhere for each ecoregion boundary and loading the ecoregion description information online that is currently locked up in a downloadable PDF. Unfortunately, both of those objectives are a little problematic.\n",
    "\n",
    "Sticking within the Wikiverse and according to documentation, we should be able to load the geometry to Wikimedia Commons items and reference them with the geoshape property. However, after looking into where and how this has been done in other items, I really didn't see any added value to that approach. Any programming against the system would have to go through some pretty circuitous routes to put everything together for use in analysis or visualization. What we would want is some kind of immediate access to a geospatial service from the WikiData item that would return the geospatial part of the data in a variety of formats. Unfortunately, I wasn't able to track down anything close to an authoritative, stable source for that, even from our own geospatial servers in USGS. An interesting phenomenon seen online are numerous cases where people have clipped out an ecoregion subset of one kind or another at State boundaries, probably for ecosystem context and analysis within the State. I've yet to find one of those that shows a careful provenance trace to where the information comes from or how it was processed.\n",
    "\n",
    "I should be able to do something interesting with the ecoregion descriptions, but I need to think about it a while longer. The original source is in a PDF provided from the same source as the data and released in 2011 by the are a little bit in a government report type of release from the Commission for Environmental Cooperation. That document has a Creative Commons-No Derivatives license, which essentially prohibits taking pieces of it out and putting them somewhere else. However, EPA also released a Word version of the basic descriptive content that should be completely public domain, and I could conceivably build some code to scrape the individual descriptions and create Wikimedia Commons articles for them as a reference point. That would give us a persistent identity and the content to work with. But there are a number of Wikipedia articles that mostly describe some of the same basic ecosystems, albeit from a different perspective with Omernik's work just one source. I haven't figured out the most elegant way to link these various artifacts together just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from functions import get_source_df, lookup_wd\n",
    "import os\n",
    "import copy\n",
    "from wikidataintegrator import wdi_helpers, wdi_core\n",
    "import pickle\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from geoalchemy2 import Geometry, WKTElement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built all workflow steps here into a suite of functions that can be composed in a variety of ways but generally need to operate in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_source_data():\n",
    "    na_er1 = get_source_df(\n",
    "        source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/na_cec_eco_l1.zip\",\n",
    "        add_columns=[('source', 'NA_CEC_Eco_Level1')],\n",
    "        target_file_name=\"NA_CEC_Eco_Level1.shp\"\n",
    "    )\n",
    "\n",
    "    na_er2 = get_source_df(\n",
    "        source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/na_cec_eco_l2.zip\",\n",
    "        add_columns=[('source', 'NA_CEC_Eco_Level2')],\n",
    "        target_file_name=\"NA_CEC_Eco_Level2.shp\"\n",
    "    )\n",
    "\n",
    "    na_er3 = get_source_df(\n",
    "        source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/NA_CEC_Eco_Level3.zip\",\n",
    "        add_columns=[('source', 'NA_CEC_Eco_Level3')]\n",
    "    )\n",
    "\n",
    "    conus_er3 = get_source_df(\n",
    "        source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\",\n",
    "        add_columns=[('source', 'US_Eco_Level3')]\n",
    "    )\n",
    "\n",
    "    conus_er4 = get_source_df(\n",
    "        source_url=\"ftp://newftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip\",\n",
    "        add_columns=[('source', 'US_Eco_Level4')],\n",
    "        target_file_name=\"us_eco_l4_no_st.shp\"\n",
    "    )\n",
    "    \n",
    "    na_er1 = na_er1.to_crs({'init': 'epsg:4326'})\n",
    "    na_er2 = na_er2.to_crs({'init': 'epsg:4326'})\n",
    "    na_er3 = na_er3.to_crs({'init': 'epsg:4326'})\n",
    "    conus_er3 = conus_er3.to_crs({'init': 'epsg:4326'})\n",
    "    conus_er4 = conus_er4.to_crs({'init': 'epsg:4326'})\n",
    "    \n",
    "    return na_er1, na_er2, na_er3, conus_er3, conus_er4\n",
    "\n",
    "def clean_synth_df(na_er1, na_er2, na_er3, conus_er3, conus_er4):\n",
    "    na_er1[\"contextual_identifier\"] = na_er1.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE}\", axis=1)\n",
    "    na_er1[\"common_name\"] = na_er1.apply(lambda row: row.NA_L1NAME, axis=1)\n",
    "    na_er1.drop(\n",
    "        [c for c in list(na_er1.columns) if c not in [\"source\",\"contextual_identifier\",\"common_name\",\"geometry\"]], \n",
    "        axis=1, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    na_er2[\"contextual_identifier\"] = na_er2.apply(lambda row: f\"NA_L2CODE:{row.NA_L2CODE}\", axis=1)\n",
    "    na_er2[\"common_name\"] = na_er2.apply(lambda row: row.NA_L2NAME, axis=1)\n",
    "    na_er2[\"part_of_identifiers\"] = na_er2.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE}\", axis=1)\n",
    "    na_er2.drop(\n",
    "        [c for c in list(na_er2.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "        axis=1, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    na_er3[\"contextual_identifier\"] = na_er3.apply(lambda row: f\"NA_L3CODE:{row.NA_L3CODE}\", axis=1)\n",
    "    na_er3[\"common_name\"] = na_er3.apply(lambda row: row.NA_L3NAME, axis=1)\n",
    "    na_er3[\"part_of_identifiers\"] = na_er3.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE};NA_L2CODE:{row.NA_L2CODE}\", axis=1)\n",
    "    na_er3.drop(\n",
    "        [c for c in list(na_er3.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "        axis=1, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    conus_er3[\"contextual_identifier\"] = conus_er3.apply(lambda row: f\"US_L3CODE:{row.US_L3CODE}\", axis=1)\n",
    "    conus_er3[\"common_name\"] = conus_er3.apply(lambda row: row.US_L3NAME, axis=1)\n",
    "    conus_er3[\"part_of_identifiers\"] = conus_er3.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE};NA_L2CODE:{row.NA_L2CODE};NA_L3CODE:{row.NA_L3CODE}\", axis=1)\n",
    "    conus_er3.drop(\n",
    "        [c for c in list(conus_er3.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "        axis=1, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    conus_er4[\"contextual_identifier\"] = conus_er4.apply(lambda row: f\"US_L4CODE:{row.US_L4CODE}\", axis=1)\n",
    "    conus_er4[\"common_name\"] = conus_er4.apply(lambda row: row.US_L4NAME, axis=1)\n",
    "    conus_er4[\"part_of_identifiers\"] = conus_er4.apply(lambda row: f\"NA_L1CODE:{row.NA_L1CODE};NA_L2CODE:{row.NA_L2CODE};NA_L3CODE:{row.NA_L3CODE};US_L3CODE:{row.US_L3CODE}\", axis=1)\n",
    "    conus_er4.drop(\n",
    "        [c for c in list(conus_er4.columns) if c not in [\"source\",\"contextual_identifier\",\"part_of_identifiers\",\"common_name\",\"geometry\"]], \n",
    "        axis=1, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    return na_er1, na_er2, na_er3, conus_er3, conus_er4\n",
    "\n",
    "def combine_source_data(na_er1, na_er2, na_er3, conus_er3, conus_er4):\n",
    "    ecoregions = gpd.GeoDataFrame(\n",
    "        pd.concat(\n",
    "            [\n",
    "                na_er1, \n",
    "                na_er2, \n",
    "                na_er3, \n",
    "                conus_er3,\n",
    "                conus_er4\n",
    "            ], \n",
    "            ignore_index=True, \n",
    "            sort=False\n",
    "        )\n",
    "    )\n",
    "    ecoregions.crs = na_er1.crs\n",
    "    \n",
    "    return ecoregions\n",
    "\n",
    "def combine_geometries(combined_source):\n",
    "    return combined_source.dissolve(by='contextual_identifier')\n",
    "\n",
    "def prepare_wikidata_info(combined_source):\n",
    "    combined_source.reset_index(level=0, inplace=True)\n",
    "\n",
    "    combined_source[\"x\"] = combined_source.centroid.x\n",
    "    combined_source[\"y\"] = combined_source.centroid.y\n",
    "\n",
    "    combined_source['common_name'] = combined_source['common_name'].str.title()\n",
    "    combined_source[\"wikidata_id\"] = combined_source.loc[\n",
    "        combined_source.source.isin([\"US_Eco_Level3\",\"US_Eco_Level4\"])\n",
    "    ].apply(lambda row: lookup_wd(row[\"contextual_identifier\"]), axis=1)\n",
    "    combined_source = combined_source.where(pd.notnull(combined_source), None)\n",
    "    \n",
    "    return combined_source\n",
    "    \n",
    "def cache_source(combined_source):\n",
    "    combined_source.to_file(\"data_cache/ecoregions_integrated.gpkg\", driver=\"GPKG\")\n",
    "    \n",
    "def load_cached_source():\n",
    "    return gpd.read_file(\"data_cache/ecoregions_integrated.gpkg\")\n",
    "\n",
    "def prep_source_for_pg(combined_dataset):\n",
    "    dataset_multi = combined_dataset.apply(copy.deepcopy)\n",
    "    dataset_poly = combined_dataset.apply(copy.deepcopy)\n",
    "    dataset_multi.drop(\n",
    "        dataset_multi[dataset_multi.geometry.geom_type == \"Polygon\"].index, \n",
    "        inplace=True\n",
    "    )\n",
    "    dataset_poly.drop(\n",
    "        dataset_poly[dataset_poly.geometry.geom_type == \"MultiPolygon\"].index, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    dataset_multi['geom'] = dataset_multi['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "    dataset_poly['geom'] = dataset_poly['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "\n",
    "    dataset_multi.drop('geometry', 1, inplace=True)\n",
    "    dataset_poly.drop('geometry', 1, inplace=True)\n",
    "    \n",
    "    return dataset_multi, dataset_poly\n",
    "\n",
    "def get_db_engine():\n",
    "    db_connection_url = f\"postgres://{os.environ.get('PG_USER')}:{os.environ.get('PG_PASS')}@{os.environ.get('PG_HOST')}:{os.environ.get('PG_PORT')}/ecoregions\";\n",
    "    return create_engine(db_connection_url)\n",
    "\n",
    "def load_to_pg(dataset_multi, dataset_poly):\n",
    "    db_engine = get_db_engine()\n",
    "    \n",
    "    dataset_poly.to_sql(\n",
    "        \"ecoregions_p\", \n",
    "        db_engine, \n",
    "        if_exists='replace', \n",
    "        index=True,\n",
    "        dtype={'geom': Geometry('POLYGON', srid=4326)}\n",
    "    )\n",
    "\n",
    "    dataset_multi.to_sql(\n",
    "        \"ecoregions_m\", \n",
    "        db_engine, \n",
    "        if_exists='replace', \n",
    "        index=True,\n",
    "        dtype={'geom': Geometry('MULTIPOLYGON', srid=4326)}\n",
    "    )\n",
    "    \n",
    "def get_intersections():\n",
    "    return pd.read_sql(\"SELECT * FROM intersections\", get_db_engine())\n",
    "\n",
    "def boundary_intersects_list(contextual_identifier, intersect_type=\"admin_id\"):\n",
    "    intersecting_ids = df_intersections.loc[df_intersections.contextual_identifier == contextual_identifier][intersect_type].to_list()\n",
    "    intersecting_ids = list(set(intersecting_ids))\n",
    "    return intersecting_ids\n",
    "\n",
    "def recombine_ecoregions():\n",
    "    db_engine = get_db_engine()\n",
    "    \n",
    "    q_ecoregions = '''SELECT \n",
    "        contextual_identifier,\n",
    "        wikidata_id,\n",
    "        source, \n",
    "        common_name, \n",
    "        part_of_identifiers, \n",
    "        x, \n",
    "        y \n",
    "        FROM ecoregions_m\n",
    "        UNION\n",
    "        SELECT \n",
    "        contextual_identifier, \n",
    "        wikidata_id,\n",
    "        source, \n",
    "        common_name, \n",
    "        part_of_identifiers, \n",
    "        x, \n",
    "        y \n",
    "        FROM ecoregions_p\n",
    "        '''\n",
    "\n",
    "    df_ecoregions = pd.read_sql(q_ecoregions, db_engine)\n",
    "    df_ecoregions['part_of_identifiers'] = df_ecoregions['part_of_identifiers'].apply(lambda x: x.split(';') if x is not None else x)\n",
    "    df_ecoregions['admin_intersects'] = df_ecoregions['contextual_identifier'].apply(lambda x: boundary_intersects_list(x, \"admin_id\"))\n",
    "    df_ecoregions['country_intersects'] = df_ecoregions['contextual_identifier'].apply(lambda x: boundary_intersects_list(x, \"country_id\"))\n",
    "    \n",
    "    return df_ecoregions\n",
    "\n",
    "def cache_final_output(final_data):\n",
    "    outfile = open(\"data_cache/ecoregions.pkl\", \"wb\")\n",
    "    pickle.dump(final_data.to_dict(orient=\"records\"), outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built some helper functions to deal with common chores in this workflow, including functions to retrieve source data from FTP or HTTP URLs, run some common transformation steps, and return geodataframes for processing. The following function retrieves all 5 ecoregion source datasets as shapefiles and runs basic transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 1.23 s, total: 27.2 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "na_er1, na_er2, na_er3, conus_er3, conus_er4 = get_source_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, I simplify and standardize the data schemas of each ecoregion source dataset so to make things simpler for further processing into WikiData items. One of the main things here is setting up each level of the inherently hierarchical system with the ecoregion identifiers that will eventually become links to the related items in WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 546 ms, sys: 7.78 ms, total: 554 ms\n",
      "Wall time: 563 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "na_er1, na_er2, na_er3, conus_er3, conus_er4 = clean_synth_df(na_er1, na_er2, na_er3, conus_er3, conus_er4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this codeblock, I concatenate all ecoregion source data together into one dataframe now that everything has the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.68 ms, sys: 1.5 ms, total: 9.18 ms\n",
      "Wall time: 8.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoregions = combine_source_data(na_er1, na_er2, na_er3, conus_er3, conus_er4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subsequent steps, I need to get a representational point for each ecoregion unit and run intersections with administrative boundaries. To make this a bit simpler in providing values grouped/centered on each distinct ecoregion unit, I run a dissolve and group by the contextual_identifier created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 6.12 s, total: 2min 50s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoregions = combine_geometries(ecoregions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I introduce x and y coordinates for a representational point to add to the WikiData items, proper case the ecoregion name for the WikiData label, and run a lookup on a previously built WikiData reference set where I pulled in all current instances of Level 3 and 4 CONUS ecoregions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.4 s, sys: 236 ms, total: 5.64 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoregions = prepare_wikidata_info(ecoregions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have a fairly reasonable source dataset that has everything in it except for the computationally intensive part of intersecting with administrative boundaries. To avoid having to run all of the above processes again, I cache out an intermediary product that can be reloaded as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextual_identifier</th>\n",
       "      <th>geometry</th>\n",
       "      <th>source</th>\n",
       "      <th>common_name</th>\n",
       "      <th>part_of_identifiers</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>wikidata_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA_L1CODE:0</td>\n",
       "      <td>MULTIPOLYGON (((-103.02853 20.33413, -103.0274...</td>\n",
       "      <td>NA_CEC_Eco_Level1</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>-84.816046</td>\n",
       "      <td>45.157872</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA_L1CODE:1</td>\n",
       "      <td>MULTIPOLYGON (((-62.19257 57.96496, -62.21423 ...</td>\n",
       "      <td>NA_CEC_Eco_Level1</td>\n",
       "      <td>Arctic Cordillera</td>\n",
       "      <td>None</td>\n",
       "      <td>-75.593162</td>\n",
       "      <td>75.479858</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA_L1CODE:10</td>\n",
       "      <td>MULTIPOLYGON (((-115.89383 46.13509, -115.8900...</td>\n",
       "      <td>NA_CEC_Eco_Level1</td>\n",
       "      <td>North American Deserts</td>\n",
       "      <td>None</td>\n",
       "      <td>-111.603628</td>\n",
       "      <td>36.294396</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NA_L1CODE:11</td>\n",
       "      <td>MULTIPOLYGON (((-115.15855 28.21240, -115.1499...</td>\n",
       "      <td>NA_CEC_Eco_Level1</td>\n",
       "      <td>Mediterranean California</td>\n",
       "      <td>None</td>\n",
       "      <td>-119.719574</td>\n",
       "      <td>35.919024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NA_L1CODE:12</td>\n",
       "      <td>MULTIPOLYGON (((-103.54038 22.09263, -103.5405...</td>\n",
       "      <td>NA_CEC_Eco_Level1</td>\n",
       "      <td>Southern Semiarid Highlands</td>\n",
       "      <td>None</td>\n",
       "      <td>-105.577253</td>\n",
       "      <td>26.395076</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  contextual_identifier                                           geometry  \\\n",
       "0           NA_L1CODE:0  MULTIPOLYGON (((-103.02853 20.33413, -103.0274...   \n",
       "1           NA_L1CODE:1  MULTIPOLYGON (((-62.19257 57.96496, -62.21423 ...   \n",
       "2          NA_L1CODE:10  MULTIPOLYGON (((-115.89383 46.13509, -115.8900...   \n",
       "3          NA_L1CODE:11  MULTIPOLYGON (((-115.15855 28.21240, -115.1499...   \n",
       "4          NA_L1CODE:12  MULTIPOLYGON (((-103.54038 22.09263, -103.5405...   \n",
       "\n",
       "              source                  common_name part_of_identifiers  \\\n",
       "0  NA_CEC_Eco_Level1                        Water                None   \n",
       "1  NA_CEC_Eco_Level1            Arctic Cordillera                None   \n",
       "2  NA_CEC_Eco_Level1       North American Deserts                None   \n",
       "3  NA_CEC_Eco_Level1     Mediterranean California                None   \n",
       "4  NA_CEC_Eco_Level1  Southern Semiarid Highlands                None   \n",
       "\n",
       "            x          y wikidata_id  \n",
       "0  -84.816046  45.157872        None  \n",
       "1  -75.593162  75.479858        None  \n",
       "2 -111.603628  36.294396        None  \n",
       "3 -119.719574  35.919024        None  \n",
       "4 -105.577253  26.395076        None  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecoregions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 s, sys: 4.27 s, total: 38 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cache_source(ecoregions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I load the two datasets of like geometry to PostGIS tables for spatial processing to find intersections with administrative boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 794 ms, total: 26.7 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_multi, dataset_poly = prep_source_for_pg(ecoregions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 s, sys: 1.51 s, total: 4.08 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_to_pg(dataset_multi, dataset_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this part of things out for further processing into WikiData items, I used processes directly in PostGIS to have it handle the more intensive spatial processing. This involved creating a materialized view containing the names and identifiers of intersected boundaries that can be used to generate WikiData links (statements/claims) to relevant items. This was handled with the following SQL.\n",
    "\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_m AS b, ecoregions_m AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "    UNION\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_m AS b, ecoregions_p AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "    UNION\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_p AS b, ecoregions_m AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "    UNION\n",
    "    SELECT b.name AS boundary_name, \n",
    "    b.wikidata_id AS wikidata_id,\n",
    "    e.contextual_identifier\n",
    "    FROM boundaries_p AS b, ecoregions_p AS e\n",
    "    WHERE ST_Intersects(b.geom, e.geom)\n",
    "\n",
    "Note: I do need to eventually come up with a more elegant and performant way of handling this type of linking functionality using cloud resources somewhere as it is a fairly common task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With intersections discovered, we can bring everything together into one combined dataset that contains the essential information we will use to create the WikiData items. The following codeblock retrieves the intersection data from PostGIS and sets up a simple function to retrieve the WikiData ID values for those admin entities that I previously assembled whose geometry intersects with ecoregion boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intersections = get_intersections()\n",
    "df_ecoregions = recombine_ecoregions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I bring the properties (minus geometry) together again for all ecoregion units across all 5 datasets and apply a couple of processes to prep the data for working up into WikiData items. These include splitting the list of other ecoregion identifiers that will become \"part of\" relationships in WikiData into a list object and applying our function to provide a list of intersecting administrative boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have everything we need to stub out functional WikiData items for each ecoregion unit in all 3 North American classification systems and the 2 for CONUS. This includes linkages we will be able to create to state/province administrative units across all three countries (and each of the countries) along with US counties. We have a representational point location for basic map orientation. And we will be able to build out \"part of/has part\" relationships between relevant ecoregion units once we have the items created and IDs generated in WikiData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save out the final processed dataset as a Python data structure for processing to WikiData in another step. At this point, we no longer need our PostGIS intermediary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextual_identifier</th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>source</th>\n",
       "      <th>common_name</th>\n",
       "      <th>part_of_identifiers</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>admin_intersects</th>\n",
       "      <th>country_intersects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US_L4CODE:50aa</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Menominee-Drummond Lakeshore</td>\n",
       "      <td>[NA_L1CODE:5, NA_L2CODE:5.2, NA_L3CODE:5.2.1, ...</td>\n",
       "      <td>-85.871227</td>\n",
       "      <td>45.918197</td>\n",
       "      <td>[Q491857, Q1166, Q491882, Q490477, Q1130480, Q...</td>\n",
       "      <td>[Q30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US_L4CODE:51g</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Door Peninsula</td>\n",
       "      <td>[NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.4, ...</td>\n",
       "      <td>-87.327961</td>\n",
       "      <td>44.916542</td>\n",
       "      <td>[Q932951, Q1537, Q490550]</td>\n",
       "      <td>[Q30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA_L3CODE:9.4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NA_CEC_Eco_Level3</td>\n",
       "      <td>Cross Timbers</td>\n",
       "      <td>[NA_L1CODE:9, NA_L2CODE:9.4]</td>\n",
       "      <td>-97.506560</td>\n",
       "      <td>33.588485</td>\n",
       "      <td>[Q484603, Q484586, Q495867, Q495873, Q110500, ...</td>\n",
       "      <td>[Q30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US_L4CODE:63b</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Chesapeake-Pamlico Lowlands And Tidal Marshes</td>\n",
       "      <td>[NA_L1CODE:8, NA_L2CODE:8.5, NA_L3CODE:8.5.1, ...</td>\n",
       "      <td>-76.286714</td>\n",
       "      <td>36.985518</td>\n",
       "      <td>[Q511106, Q504294, Q488701, Q497817, Q49289, Q...</td>\n",
       "      <td>[Q30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US_L4CODE:59c</td>\n",
       "      <td>None</td>\n",
       "      <td>US_Eco_Level4</td>\n",
       "      <td>Southern New England Coastal Plains And Hills</td>\n",
       "      <td>[NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.7, ...</td>\n",
       "      <td>-72.398028</td>\n",
       "      <td>41.656111</td>\n",
       "      <td>[Q54235, Q115266, Q54253, Q1384, Q771, Q54251,...</td>\n",
       "      <td>[Q30]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  contextual_identifier wikidata_id             source  \\\n",
       "0        US_L4CODE:50aa        None      US_Eco_Level4   \n",
       "1         US_L4CODE:51g        None      US_Eco_Level4   \n",
       "2       NA_L3CODE:9.4.5        None  NA_CEC_Eco_Level3   \n",
       "3         US_L4CODE:63b        None      US_Eco_Level4   \n",
       "4         US_L4CODE:59c        None      US_Eco_Level4   \n",
       "\n",
       "                                     common_name  \\\n",
       "0                   Menominee-Drummond Lakeshore   \n",
       "1                                 Door Peninsula   \n",
       "2                                  Cross Timbers   \n",
       "3  Chesapeake-Pamlico Lowlands And Tidal Marshes   \n",
       "4  Southern New England Coastal Plains And Hills   \n",
       "\n",
       "                                 part_of_identifiers          x          y  \\\n",
       "0  [NA_L1CODE:5, NA_L2CODE:5.2, NA_L3CODE:5.2.1, ... -85.871227  45.918197   \n",
       "1  [NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.4, ... -87.327961  44.916542   \n",
       "2                       [NA_L1CODE:9, NA_L2CODE:9.4] -97.506560  33.588485   \n",
       "3  [NA_L1CODE:8, NA_L2CODE:8.5, NA_L3CODE:8.5.1, ... -76.286714  36.985518   \n",
       "4  [NA_L1CODE:8, NA_L2CODE:8.1, NA_L3CODE:8.1.7, ... -72.398028  41.656111   \n",
       "\n",
       "                                    admin_intersects country_intersects  \n",
       "0  [Q491857, Q1166, Q491882, Q490477, Q1130480, Q...              [Q30]  \n",
       "1                          [Q932951, Q1537, Q490550]              [Q30]  \n",
       "2  [Q484603, Q484586, Q495867, Q495873, Q110500, ...              [Q30]  \n",
       "3  [Q511106, Q504294, Q488701, Q497817, Q49289, Q...              [Q30]  \n",
       "4  [Q54235, Q115266, Q54253, Q1384, Q771, Q54251,...              [Q30]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ecoregions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_final_output(df_ecoregions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
